from google.colab import drive
drive.mount('/content/drive')
# Install Libraries
# !pip install boto3
# below took 2m 36s
!pip install torch torchvision
!pip install transformers datasets
!pip install transformers[torch]
!pip show accelerate
!pip install rembg
!pip install boto3
!pip install pillow
# Imports
import boto3
import io
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import numpy as np
import torch
import torchvision
from torchvision.transforms import Normalize, Resize, ToTensor, Compose
from PIL import Image
from torchvision.transforms import ToPILImage
import matplotlib.pyplot as plt

from transformers import ViTImageProcessor, ViTForImageClassification
from transformers import TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from PIL import Image
from datasets import Dataset, ClassLabel, Features

from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc

from transformers import TrainerCallback
from copy import deepcopy
# from fastapi import FastAPI
from typing import List
import urllib
import numpy as np
import torch
from torchvision import transforms
# from fastapi.middleware.cors import CORSMiddleware

import os
import shutil
from PIL import Image, UnidentifiedImageError
from skimage.transform import resize
import urllib.request
from urllib.parse import urlparse
from io import BytesIO
import rembg
from rembg import remove
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)
## Load the model
# Load the PyTorch model
# model = torch.load('whole_model.pth', map_location=torch.device('cpu'))
path = '/content/drive/MyDrive/MIDS_Capstone/Model Artifacts/ViT/vit_model.pth'
# path = 'vit_model.pth'
# path = '/content/drive/MyDrive/Claire/vit_model.pth'
vit_model = torch.load(path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vit_model.to(device)
vit_model.eval()
# Preprocessing Code
# DATA PREPROCESSING SCRIPT FROM ADA
def resize_with_padding(image, target_height, target_width):
    """
    Resize image with padding to maintain aspect ratio.

    """
    # Calculate aspect ratio of original image
    original_height, original_width, _ = image.shape
    aspect_ratio = original_width / original_height

    # Resize image while preserving aspect ratio and fill with white pixels
    if aspect_ratio > target_width / target_height:
        # Image is wider, resize based on width
        new_width = target_width
        new_height = int(target_width / aspect_ratio)
    elif aspect_ratio < target_width / target_height:
        # Image is taller, resize based on height
        new_height = target_height
        new_width = int(aspect_ratio * target_height)
    else:
        # Image has the same aspect ratio as target
        new_height = target_height
        new_width = target_width

    resized_image = resize(image, (new_height, new_width), mode='constant') * 255  # Fill with white pixels

    # Pad to target dimensions with white pixels if necessary
    padded_image = np.ones((target_height, target_width, 3), dtype=np.uint8) * 255  # Fill with white pixels
    y_offset = (target_height - new_height) // 2
    x_offset = (target_width - new_width) // 2
    padded_image[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized_image

    return padded_image.astype(np.uint8)
def remove_transparency(im, bg_colour=(255, 255, 255)):
    # Only process if image has transparency (http://stackoverflow.com/a/1963146)
    if im.mode in ('RGBA', 'LA') or (im.mode == 'P' and 'transparency' in im.info):

        # Need to convert to RGBA if LA format due to a bug in PIL (http://stackoverflow.com/a/1963146)
        alpha = im.convert('RGBA').split()[-1]

        # Create a new background image of our matt color.
        # Must be RGBA because paste requires both images have the same format
        # (http://stackoverflow.com/a/8720632  and  http://stackoverflow.com/a/9459208)
        bg = Image.new("RGBA", im.size, bg_colour + (255,))
        bg.paste(im, mask=alpha)
        return bg.convert('RGB')

    else:
        return im
def process_image(image_url, target_height, target_width):
    # Download the image from the URL
    with urllib.request.urlopen(image_url) as response:
        img_data = response.read()

    # Open the image using PIL
    with Image.open(BytesIO(img_data)) as img:
        # Remove background
        img = remove(img, bgcolor=[0,0,0,255])
        img = img.convert("RGB")
    # with Image.open(img_data) as img:
        # Remove transparency and resize the image
        img = remove_transparency(img)
        # new_size = (224, 224)  # Adjust the new size as needed
        # img = img.resize(new_size).convert('RGB')
        img_array = np.array(img).astype(np.uint8)

        # Resize and pad the image
        processed_image = resize_with_padding(img_array, target_height, target_width)

        # Normalize the image
        processed_image = processed_image.astype(np.float32) / 255.0


    return processed_image
def process_image_backgroundincluded(image_url, target_height, target_width):
    # Download the image from the URL
    with urllib.request.urlopen(image_url) as response:
        img_data = response.read()

    # Open the image using PIL
    with Image.open(BytesIO(img_data)) as img:
        # Remove background
        img = img.convert("RGB")
    # with Image.open(img_data) as img:
        # Remove transparency and resize the image
        img = remove_transparency(img)
        # new_size = (224, 224)  # Adjust the new size as needed
        # img = img.resize(new_size).convert('RGB')
        img_array = np.array(img).astype(np.uint8)

        # Resize and pad the image
        processed_image = resize_with_padding(img_array, target_height, target_width)

        # Normalize the image
        processed_image = processed_image.astype(np.float32) / 255.0


    return processed_image
## Preprocess Image
url = "https://consumerecology.com/wp-content/uploads/2021/05/Cardboard-Box-1200.jpg"
# url = 'data:image/jpeg;base64,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'
image_array = process_image(url, 224, 224)
plt.imshow(image_array)
## Convert image array to tensor
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()

## Call Model
results = []
labels = ['battery', 'beverage cans', 'cardboard', 'cigarette butt',
       'construction scrap', 'electrical cables', 'electronic chips',
       'glass', 'gloves', 'laptops', 'masks', 'medicines',
       'metal containers', 'news paper', 'paper',
       'paper_cups', 'plastic bags', 'plastic bottles',
       'plastic containers', 'plastic_cups', 'small appliances',
       'smartphones', 'spray cans', 'syringe', 'tetra pak', 'trash']
general_recycling = {
    'battery': 'battery',
    'beverage cans': 'metal',
    'cardboard': 'cardboard',
    'cigarette butt': 'plastic',
    'construction scrap': 'metal',
    'electrical cables': 'e-waste',
    'electronic chips': 'e-waste',
    'glass': 'glass',
    'gloves': 'medical',
    'laptops': 'e-waste',
    'masks': 'medical',
    'medicines': 'medical',
    'metal containers': 'metal',
    'news paper': 'paper',
    'paper': 'paper',
    'paper_cups': 'paper',
    'plastic bags': 'plastic',
    'plastic bottles': 'plastic',
    'plastic containers': 'plastic',
    'plastic_cups': 'plastic',
    'small appliances': 'e-waste',
    'smartphones': 'e-waste',
    'spray cans': 'metal',
    'syringe': 'medical',
    'tetra pak': 'paper',
    'trash': 'trash'
}
# image_tensor_unsqueezed = image_tensor_normalized.unsqueeze(0)
# image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# # Perform prediction
# with torch.no_grad():
#     input = image_tensor_transposed.to(device)
#     output = vit_model(input)
#     logits = output.logits
#     predicted_label = torch.argmax(logits, 1)
#     softmax_probs = torch.softmax(logits, 1)
#     softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# # Retrieve string for predicted label
# subcategory = labels[predicted_label]

# # Retrieve general recycling category for predicted label
# gen_label = general_recycling[subcategory]

# # Set is_recyclable
# is_recyclable = subcategory != 'trash'

# # Subcategory recommendation
# subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# # Set confidence category
# if softmax_prob <= 0.5:
#     confidence_category = "low"
# elif 0.5 < softmax_prob < 0.75:
#     confidence_category = "medium"
# else:
#     confidence_category = "high"

# print("image_url: ", url)
# # print("filename: ", filename)
# print("wastetype_raw: ", subcategory)
# print("wastetype_suggested: ", subcategory_sug)
# print("wastetype_general: ", gen_label)
# print("is_recyclable: ", is_recyclable)
# print("confidence_category: ", confidence_category)
# print("softmax_logit: ", np.round(softmax_prob, 6)[0])
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

## TEST WITH IMAGES FROM GOOGLE DRIVE
# Test with Cardboard
import requests
from PIL import Image
import matplotlib.pyplot as plt

# https://drive.google.com/file/d/1Y62v0Ck9j8RnjOn4am1QAFMmmSa9a3GI/view?usp=drive_link # cardboard 43
# https://drive.google.com/file/d/1UmOwCCYBHFHvWDNsNo6W9jNs73PXRh9V/view?usp=drive_link # cardboard 305
# https://drive.google.com/file/d/1AvZYLwWoFoJPcUA1J2y5pknVmfzs9JU5/view?usp=drive_link # smartphone 5 - MISCLASSIFICATION
# https://drive.google.com/file/d/1KIyAfI2qyyASSHNWdRqSdVC964yHXHRQ/view?usp=drive_link # masks 41 - MISCLASSIFICATION
id = "1Y62v0Ck9j8RnjOn4am1QAFMmmSa9a3GI"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()

# url = 'data:image/jpeg;base64,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'
image_array = process_image(url, 224, 224)
plt.imshow(image_array)
## Convert image array to tensor
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
## Call Model
results = []
labels = ['battery', 'beverage cans', 'cardboard', 'cigarette butt',
       'construction scrap', 'electrical cables', 'electronic chips',
       'glass', 'gloves', 'laptops', 'masks', 'medicines',
       'metal containers', 'news paper', 'paper',
       'paper_cups', 'plastic bags', 'plastic bottles',
       'plastic containers', 'plastic_cups', 'small appliances',
       'smartphones', 'spray cans', 'syringe', 'tetra pak', 'trash']
general_recycling = {
    'battery': 'battery',
    'beverage cans': 'metal',
    'cardboard': 'cardboard',
    'cigarette butt': 'plastic',
    'construction scrap': 'metal',
    'electrical cables': 'e-waste',
    'electronic chips': 'e-waste',
    'glass': 'glass',
    'gloves': 'medical',
    'laptops': 'e-waste',
    'masks': 'medical',
    'medicines': 'medical',
    'metal containers': 'metal',
    'news paper': 'paper',
    'paper': 'paper',
    'paper_cups': 'paper',
    'plastic bags': 'plastic',
    'plastic bottles': 'plastic',
    'plastic containers': 'plastic',
    'plastic_cups': 'plastic',
    'small appliances': 'e-waste',
    'smartphones': 'e-waste',
    'spray cans': 'metal',
    'syringe': 'medical',
    'tetra pak': 'paper',
    'trash': 'trash'
}
# image_tensor_unsqueezed = image_tensor_normalized.unsqueeze(0)
# image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# # Perform prediction
# with torch.no_grad():
#     input = image_tensor_transposed.to(device)
#     output = vit_model(input)
#     logits = output.logits
#     predicted_label = torch.argmax(logits, 1)
#     softmax_probs = torch.softmax(logits, 1)
#     softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# # Retrieve string for predicted label
# subcategory = labels[predicted_label]

# # Retrieve general recycling category for predicted label
# gen_label = general_recycling[subcategory]

# # Set is_recyclable
# is_recyclable = subcategory != 'trash'

# # Subcategory recommendation
# subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# # Set confidence category
# if softmax_prob <= 0.5:
#     confidence_category = "low"
# elif 0.5 < softmax_prob < 0.75:
#     confidence_category = "medium"
# else:
#     confidence_category = "high"

# print("image_url: ", url)
# # print("filename: ", filename)
# print("wastetype_raw: ", subcategory)
# print("wastetype_suggested: ", subcategory_sug)
# print("wastetype_general: ", gen_label)
# print("is_recyclable: ", is_recyclable)
# print("confidence_category: ", confidence_category)
# print("softmax_logit: ", np.round(softmax_prob, 6)[0])
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

# Test with Glove
import requests
from PIL import Image
import matplotlib.pyplot as plt

#  https://drive.google.com/file/d/1Bxbyh2olPBqN8P5wAAP4wSV8EILGMIZW/view?usp=sharing # cardboard 3
# https://drive.google.com/file/d/1UmOwCCYBHFHvWDNsNo6W9jNs73PXRh9V/view?usp=drive_link # cardboard 305
# https://drive.google.com/file/d/1AvZYLwWoFoJPcUA1J2y5pknVmfzs9JU5/view?usp=drive_link # smartphone 5 - MISCLASSIFICATION
# https://drive.google.com/file/d/1KIyAfI2qyyASSHNWdRqSdVC964yHXHRQ/view?usp=drive_link # masks 41 - MISCLASSIFICATION
# https://drive.google.com/file/d/1zZd5-BtYPGlHPmOqVbfhw7Z0LmVy5a1u/view?usp=drive_link # gloves 135
id = "1zZd5-BtYPGlHPmOqVbfhw7Z0LmVy5a1u"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()

# url = 'data:image/jpeg;base64,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'

image_array = process_image(url, 224, 224)
plt.imshow(image_array)
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
## Convert image array to tensor
## Call Model
# image_tensor_unsqueezed = image_tensor_normalized.unsqueeze(0)
# image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# # Perform prediction
# with torch.no_grad():
#     input = image_tensor_transposed.to(device)
#     output = vit_model(input)
#     logits = output.logits
#     predicted_label = torch.argmax(logits, 1)
#     softmax_probs = torch.softmax(logits, 1)
#     softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# # Retrieve string for predicted label
# subcategory = labels[predicted_label]

# # Retrieve general recycling category for predicted label
# gen_label = general_recycling[subcategory]

# # Set is_recyclable
# is_recyclable = subcategory != 'trash'

# # Subcategory recommendation
# subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# # Set confidence category
# if softmax_prob <= 0.5:
#     confidence_category = "low"
# elif 0.5 < softmax_prob < 0.75:
#     confidence_category = "medium"
# else:
#     confidence_category = "high"

# print("image_url: ", url)
# # print("filename: ", filename)
# print("wastetype_raw: ", subcategory)
# print("wastetype_suggested: ", subcategory_sug)
# print("wastetype_general: ", gen_label)
# print("is_recyclable: ", is_recyclable)
# print("confidence_category: ", confidence_category)
# print("softmax_logit: ", np.round(softmax_prob, 6)[0])
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

# Test with Mask (BAD result)
import requests
from PIL import Image
import matplotlib.pyplot as plt

#  https://drive.google.com/file/d/1Bxbyh2olPBqN8P5wAAP4wSV8EILGMIZW/view?usp=sharing # cardboard 3
# https://drive.google.com/file/d/1UmOwCCYBHFHvWDNsNo6W9jNs73PXRh9V/view?usp=drive_link # cardboard 305
# https://drive.google.com/file/d/1AvZYLwWoFoJPcUA1J2y5pknVmfzs9JU5/view?usp=drive_link # smartphone 5 - MISCLASSIFICATION
# https://drive.google.com/file/d/1KIyAfI2qyyASSHNWdRqSdVC964yHXHRQ/view?usp=drive_link # masks 41 - MISCLASSIFICATION
id = "1KIyAfI2qyyASSHNWdRqSdVC964yHXHRQ"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()

# url = 'data:image/jpeg;base64,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'

image_array = process_image(url, 224, 224)
plt.imshow(image_array)
## Convert image array to tensor
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
## Call Model
# image_tensor_unsqueezed = image_tensor_normalized.unsqueeze(0)
# image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# # Perform prediction
# with torch.no_grad():
#     input = image_tensor_transposed.to(device)
#     output = vit_model(input)
#     logits = output.logits
#     predicted_label = torch.argmax(logits, 1)
#     softmax_probs = torch.softmax(logits, 1)
#     softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# # Retrieve string for predicted label
# subcategory = labels[predicted_label]

# # Retrieve general recycling category for predicted label
# gen_label = general_recycling[subcategory]

# # Set is_recyclable
# is_recyclable = subcategory != 'trash'

# # Subcategory recommendation
# subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# # Set confidence category
# if softmax_prob <= 0.5:
#     confidence_category = "low"
# elif 0.5 < softmax_prob < 0.75:
#     confidence_category = "medium"
# else:
#     confidence_category = "high"

# print("image_url: ", url)
# # print("filename: ", filename)
# print("wastetype_raw: ", subcategory)
# print("wastetype_suggested: ", subcategory_sug)
# print("wastetype_general: ", gen_label)
# print("is_recyclable: ", is_recyclable)
# print("confidence_category: ", confidence_category)
# print("softmax_logit: ", np.round(softmax_prob, 6)[0])
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

# Test with Smartphone (BAD result)
import requests
from PIL import Image
import matplotlib.pyplot as plt

#  https://drive.google.com/file/d/1Bxbyh2olPBqN8P5wAAP4wSV8EILGMIZW/view?usp=sharing # cardboard 3
# https://drive.google.com/file/d/1UmOwCCYBHFHvWDNsNo6W9jNs73PXRh9V/view?usp=drive_link # cardboard 305
# https://drive.google.com/file/d/1AvZYLwWoFoJPcUA1J2y5pknVmfzs9JU5/view?usp=drive_link # smartphone 5 - MISCLASSIFICATION
# https://drive.google.com/file/d/1KIyAfI2qyyASSHNWdRqSdVC964yHXHRQ/view?usp=drive_link # masks 41 - MISCLASSIFICATION
id = "1AvZYLwWoFoJPcUA1J2y5pknVmfzs9JU5"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()

# url = 'data:image/jpeg;base64,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'

image_array = process_image(url, 224, 224)
plt.imshow(image_array)
## Convert image array to tensor
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
## Call Model
# image_tensor_unsqueezed = image_tensor_normalized.unsqueeze(0)
# image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# # Perform prediction
# with torch.no_grad():
#     input = image_tensor_transposed.to(device)
#     output = vit_model(input)
#     logits = output.logits
#     predicted_label = torch.argmax(logits, 1)
#     softmax_probs = torch.softmax(logits, 1)
#     softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# # Retrieve string for predicted label
# subcategory = labels[predicted_label]

# # Retrieve general recycling category for predicted label
# gen_label = general_recycling[subcategory]

# # Set is_recyclable
# is_recyclable = subcategory != 'trash'

# # Subcategory recommendation
# subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# # Set confidence category
# if softmax_prob <= 0.5:
#     confidence_category = "low"
# elif 0.5 < softmax_prob < 0.75:
#     confidence_category = "medium"
# else:
#     confidence_category = "high"

# print("image_url: ", url)
# # print("filename: ", filename)
# print("wastetype_raw: ", subcategory)
# print("wastetype_suggested: ", subcategory_sug)
# print("wastetype_general: ", gen_label)
# print("is_recyclable: ", is_recyclable)
# print("confidence_category: ", confidence_category)
# print("softmax_logit: ", np.round(softmax_prob, 6)[0])
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

## Call Model

import requests
from PIL import Image
import matplotlib.pyplot as plt

# https://drive.google.com/file/d/1iTpcyXwctnCoS8zxuRQB_L0qeIFASUd_/view?usp=sharing # medicine 7

id = "1iTpcyXwctnCoS8zxuRQB_L0qeIFASUd_"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()
image_array = process_image(url, 224, 224)
plt.imshow(image_array)
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

import requests
from PIL import Image
import matplotlib.pyplot as plt

# https://drive.google.com/file/d/1z1wrzBIDkVPTIaFc28NfHXSSU7UV1zVv/view?usp=drive_link # glove 102


id = "1z1wrzBIDkVPTIaFc28NfHXSSU7UV1zVv"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()
image_array = process_image(url, 224, 224)
plt.imshow(image_array)
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

import requests
from PIL import Image
import matplotlib.pyplot as plt

# https://drive.google.com/file/d/1GuoqFqbxNP53OFiMh1ZKasSr2yh-9PTN/view?usp=drive_link # laptop 9


id = "1GuoqFqbxNP53OFiMh1ZKasSr2yh-9PTN"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()
image_array = process_image(url, 224, 224)
plt.imshow(image_array)
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

import requests
from PIL import Image
import matplotlib.pyplot as plt

# https://drive.google.com/file/d/16-f1mQbx4S3LllONad2i_7OGeQqe2vTJ/view?usp=drive_link # mask 6


id = "16-f1mQbx4S3LllONad2i_7OGeQqe2vTJ"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()
image_array = process_image(url, 224, 224)
plt.imshow(image_array)
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

import requests
from PIL import Image
import matplotlib.pyplot as plt

# https://drive.google.com/file/d/1jJRtGPFUrLBMRc6rJVoA7K769s92Fzzo/view?usp=drive_link # small appliances 18


id = "1jJRtGPFUrLBMRc6rJVoA7K769s92Fzzo"
url = "https://drive.usercontent.google.com/download?id=" + id


response = requests.get(url, stream=True)
img = Image.open(response.raw)

plt.title("Original Image")
plt.imshow(img)
plt.axis("off")  # Hide axes
plt.show()
image_array = process_image(url, 224, 224)
plt.imshow(image_array)
# Convert the image array to a PyTorch tensor
image_tensor = torch.tensor(image_array, dtype=torch.float32)

# Convert the image tensor back to a NumPy array
image_array = image_tensor.numpy()

# Display the image
plt.imshow(image_array)
plt.show()
import numpy as np
import torch

# Convert image to tensor and transpose dimensions
image_tensor_unsqueezed = image_tensor.unsqueeze(0)
image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

# Perform prediction
with torch.no_grad():
    input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
    output = vit_model(input)
    logits = output.logits
    predicted_label = torch.argmax(logits, 1)
    softmax_probs = torch.softmax(logits, 1)
    softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

# Retrieve string for predicted label
subcategory = labels[predicted_label]

# Retrieve general recycling category for predicted label
gen_label = general_recycling[subcategory]

# Set is_recyclable
is_recyclable = subcategory != 'trash'

# Subcategory recommendation
subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

# Set confidence category
if softmax_prob <= 0.5:
    confidence_category = "low"
elif 0.5 < softmax_prob < 0.75:
    confidence_category = "medium"
else:
    confidence_category = "high"

print("image_url: ", url)
# print("filename: ", filename)
print("wastetype_raw: ", subcategory)
print("wastetype_suggested: ", subcategory_sug)
print("wastetype_general: ", gen_label)
print("is_recyclable: ", is_recyclable)
print("confidence_category: ", confidence_category)
print("softmax_logit: ", np.round(softmax_prob, 6)[0])

## Experiments (with background removal)
import requests
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import torch


def test_on_new_image(id):
  # Load image
  url = "https://drive.usercontent.google.com/download?id=" + id
  response = requests.get(url, stream=True)
  img = Image.open(response.raw)

  plt.title("Original Image")
  plt.imshow(img)
  plt.axis("off")  # Hide axes
  plt.show()

  image_array = process_image(url, 224, 224)
  plt.imshow(image_array)

  # Convert the image array to a PyTorch tensor
  image_tensor = torch.tensor(image_array, dtype=torch.float32)

  # Convert the image tensor back to a NumPy array
  image_array = image_tensor.numpy()

  # Display the image
  plt.imshow(image_array)
  plt.show()

  # Convert image to tensor and transpose dimensions
  image_tensor_unsqueezed = image_tensor.unsqueeze(0)
  image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

  # Perform prediction
  with torch.no_grad():
      input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
      output = vit_model(input)
      logits = output.logits
      predicted_label = torch.argmax(logits, 1)
      softmax_probs = torch.softmax(logits, 1)
      softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

  # Retrieve string for predicted label
  subcategory = labels[predicted_label]

  # Retrieve general recycling category for predicted label
  gen_label = general_recycling[subcategory]

  # Set is_recyclable
  is_recyclable = subcategory != 'trash'

  # Subcategory recommendation
  subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

  # Set confidence category
  if softmax_prob <= 0.5:
      confidence_category = "low"
  elif 0.5 < softmax_prob < 0.75:
      confidence_category = "medium"
  else:
      confidence_category = "high"

  print("image_url: ", url)
  # print("filename: ", filename)
  print("wastetype_raw: ", subcategory)
  print("wastetype_suggested: ", subcategory_sug)
  print("wastetype_general: ", gen_label)
  print("is_recyclable: ", is_recyclable)
  print("confidence_category: ", confidence_category)
  print("softmax_logit: ", np.round(softmax_prob, 6)[0])

  return np.round(softmax_prob, 6)[0]

import re

results = []
labels = ['battery', 'beverage cans', 'cardboard', 'cigarette butt',
       'construction scrap', 'electrical cables', 'electronic chips',
       'glass', 'gloves', 'laptops', 'masks', 'medicines',
       'metal containers', 'news paper', 'paper',
       'paper_cups', 'plastic bags', 'plastic bottles',
       'plastic containers', 'plastic_cups', 'small appliances',
       'smartphones', 'spray cans', 'syringe', 'tetra pak', 'trash']
general_recycling = {
    'battery': 'battery',
    'beverage cans': 'metal',
    'cardboard': 'cardboard',
    'cigarette butt': 'plastic',
    'construction scrap': 'metal',
    'electrical cables': 'e-waste',
    'electronic chips': 'e-waste',
    'glass': 'glass',
    'gloves': 'medical',
    'laptops': 'e-waste',
    'masks': 'medical',
    'medicines': 'medical',
    'metal containers': 'metal',
    'news paper': 'paper',
    'paper': 'paper',
    'paper_cups': 'paper',
    'plastic bags': 'plastic',
    'plastic bottles': 'plastic',
    'plastic containers': 'plastic',
    'plastic_cups': 'plastic',
    'small appliances': 'e-waste',
    'smartphones': 'e-waste',
    'spray cans': 'metal',
    'syringe': 'medical',
    'tetra pak': 'paper',
    'trash': 'trash'
}

def extract_id_from_url(url):
    # Regex pattern to match the ID between 'd/' and '/view?'
    pattern = r'd/([^/]+)'
    match = re.search(pattern, url)
    # If a match is found, return the ID group
    if match:
        return match.group(1)
    else:
        return None

image_share_links = 'https://drive.google.com/file/d/1pwuQ3o7Duan3H94djfi7IBrUcIYemOj7/view?usp=drive_link, https://drive.google.com/file/d/14sCAj6-YY8otCHIgv_CzGj0vr0EnIXTK/view?usp=drive_link, https://drive.google.com/file/d/1sVIAcL3TKjTO0wQG22q-fDN-wpX_ugxc/view?usp=drive_link, https://drive.google.com/file/d/1ZtvcWuD1MIOGV4Seglst_1Q_zck5VPx2/view?usp=drive_link, https://drive.google.com/file/d/1GWSlC_y35PgbVBWerXmG5J9MwdTlBQ2E/view?usp=drive_link, https://drive.google.com/file/d/1TgtLA0AjBq6COc2MLjHtt_ATLSz_n2rw/view?usp=drive_link, https://drive.google.com/file/d/1dnIjEAyvzxV8UqvXipV_XCrmXtY2lIN0/view?usp=drive_link, https://drive.google.com/file/d/1K-NS3FyQaR8eJYQ6xc-XCxv1_JdoUeJn/view?usp=drive_link, https://drive.google.com/file/d/1pKwNyxjPr6V9uK9Ni__vJZP1lseWbltD/view?usp=drive_link, https://drive.google.com/file/d/1fVTL3EPIKCKR3I2ad1B9venKcCyjWRg6/view?usp=drive_link, https://drive.google.com/file/d/16gvLK9PF3HFBcAbHre0zGEol6WxVKDva/view?usp=drive_link, https://drive.google.com/file/d/1OUZPC08Ll1EtVG1xpTMEN8p1VZ9eRYF0/view?usp=drive_link, https://drive.google.com/file/d/1xbqroXJMBH_fNNd-NCnaSTCNY_VAT6SW/view?usp=drive_link, https://drive.google.com/file/d/185-1J1VHWWeeTjgXF6-6qSyZckHLztVd/view?usp=drive_link, https://drive.google.com/file/d/1l6p1kikwIynOqg0MUFUjh099OaBqYFBO/view?usp=drive_link, https://drive.google.com/file/d/1Nc5GvQogY4dhB0f6DiozjEmMsPdbupga/view?usp=drive_link, https://drive.google.com/file/d/1dvHUPdZeuFiR1Kv27WV-KN-klm8pnKR7/view?usp=drive_link, https://drive.google.com/file/d/1s3hG7G8AGOrNFyoSpvGUynEtMM8SIh9k/view?usp=drive_link, https://drive.google.com/file/d/1ryMZSrVpmecXFB8BlFlFgMJJyqC6Tf8N/view?usp=drive_link, https://drive.google.com/file/d/1WyBgbChLyTqjTBrq6h96lI4ZSMGq5laW/view?usp=drive_link, https://drive.google.com/file/d/145o-59Tw-y7bq9RHtUh33u_QLDW9Wq7Y/view?usp=drive_link, https://drive.google.com/file/d/1iPHilOm2BWbDCVyFf0eHVnKHiVWHCgPo/view?usp=drive_link, https://drive.google.com/file/d/1gssBv5lrrsunhZODPQw4l4ey8CezLFF3/view?usp=drive_link, https://drive.google.com/file/d/1OJntNLmV9wZURCnXWSURl9oYUnQ5WU9e/view?usp=drive_link, https://drive.google.com/file/d/1wEAn3S3-BABIXDPOJkPeP2iU6TW1L7mH/view?usp=drive_link, https://drive.google.com/file/d/1cq_bpyryPGpY_U99sP2Gjj9CK0avwkwG/view?usp=drive_link, https://drive.google.com/file/d/1JngdW7MnHXBBZhMeIzPpST4z4NpMs3Gc/view?usp=drive_link, https://drive.google.com/file/d/18yBxjwYk_Qc6U_TtrS4iUdyGCPxdXElP/view?usp=drive_link, https://drive.google.com/file/d/1luGTctak7Hg2yAWtFFkhUFB1s9QKCxx8/view?usp=drive_link, https://drive.google.com/file/d/1fo8ue-Hr4G8Q-8wiOeJZCEa9ZtqfFU8B/view?usp=drive_link, https://drive.google.com/file/d/1OGOnNi-3mqE9J2UUtI3MPrvFUxuqP7ar/view?usp=drive_link, https://drive.google.com/file/d/1bIfE8Q3XeJkTSPTVhSrL4timKLAvqNZz/view?usp=drive_link, https://drive.google.com/file/d/1Dhpjjlt9ITTSLUQQtjiEbbzGG20lK0I1/view?usp=drive_link, https://drive.google.com/file/d/1TSCM1qViy2drPxl1wO0eFU9nvxK5w79N/view?usp=drive_link, https://drive.google.com/file/d/1DgpnEoCbeRLw-UbbiaIAK5oaHg_bULq4/view?usp=drive_link, https://drive.google.com/file/d/1LXxUhJuxU7kFpLJBR8yjUxpxnM5sccZ3/view?usp=drive_link, https://drive.google.com/file/d/1_FB357gIuQDDKM6cMvZMw_7XeGXFmafa/view?usp=drive_link, https://drive.google.com/file/d/1qkjm5uLXeM-mrd1V1tZNC9t8PvlVm6Kg/view?usp=drive_link, https://drive.google.com/file/d/19-EqEaqBiiKdQ4fUi7VHLxIK726KmkLS/view?usp=drive_link, https://drive.google.com/file/d/1oZtQK9cGPikJt-pC-iT76-L5Q_L7ik_G/view?usp=drive_link, https://drive.google.com/file/d/10kn8e2HoX9752xQyUFDPgpqRVg2LwbAQ/view?usp=drive_link, https://drive.google.com/file/d/1ZXXDPiy18lkeTwYl1ThPb6WSiFWQix68/view?usp=drive_link, https://drive.google.com/file/d/1tiEo9l822N4EO4idDjH91_wqXdq9KRCT/view?usp=drive_link, https://drive.google.com/file/d/1o3I46H5oZLEl1xOI4e3n947IgtP7nAfO/view?usp=drive_link, https://drive.google.com/file/d/1uTE_3m_6VJ4r3vcVaAr7YpuQ3GRm1mpN/view?usp=drive_link, https://drive.google.com/file/d/1xyVpmDhD5ugIgLVjihBy7RoExGt24G0G/view?usp=drive_link, https://drive.google.com/file/d/19mXdguQN3EF7NUVQplTfqSX0LvMROY18/view?usp=drive_link, https://drive.google.com/file/d/1ZmerORshOWfS_eKCyWGYv9MVQdnvj0BS/view?usp=drive_link, https://drive.google.com/file/d/1N5DkliGUU9yZ2sJZM4MciNl5W42x-Fff/view?usp=drive_link, https://drive.google.com/file/d/1xsekKFqq_tmw1hxyc9gCspZVnrg74Ual/view?usp=drive_link, https://drive.google.com/file/d/1LemycgmJ6IZubj-VxXnxUhWFqg-cud2B/view?usp=drive_link, https://drive.google.com/file/d/1HsD6M1D4ZBaI5AONlnL6x5bSzWFbhnI5/view?usp=drive_link, https://drive.google.com/file/d/1rRjoKaJf9xUtu-CEEKG5c4X9L6nhfJR2/view?usp=drive_link, https://drive.google.com/file/d/1P00y728UWNWSHySp8d4d8QSyVNPyl8K5/view?usp=drive_link, https://drive.google.com/file/d/1ByYx-8gKIL2V-v2rg8L9jaciy7RYyqNk/view?usp=drive_link, https://drive.google.com/file/d/17_Dp-UxIuaMNNBL7mbwYoL8PmEd4vYdv/view?usp=drive_link, https://drive.google.com/file/d/1Fvg-SukRpINrxU2ZE9tsoUnrKUnxrRoe/view?usp=drive_link, https://drive.google.com/file/d/1oFc-SlyuPUBLCLpDCTC7eMjL716GQdz5/view?usp=drive_link, https://drive.google.com/file/d/1vs_S1aqsfAgXxuNL8kxOQsYuBKvuRDos/view?usp=drive_link, https://drive.google.com/file/d/1Ue4iyNczJxjspd4XA_23KJexLfUwiQop/view?usp=drive_link, https://drive.google.com/file/d/1ObJAZFhQUGerGi117PKkksbsHSvkFMRh/view?usp=drive_link, https://drive.google.com/file/d/1eDXobHrbaZQGUQw0M6ooHeDG-4RyYZ-v/view?usp=drive_link, https://drive.google.com/file/d/1P4_xjMBdKLfoEo-4kvFeGfJrl-L_KlAU/view?usp=drive_link, https://drive.google.com/file/d/1vPV6AitgwiwZ0WfduCan5jhtFZrLqfnc/view?usp=drive_link, https://drive.google.com/file/d/1uHPQaNfA3yiz0q-ahfUEFM4dmckuI8S9/view?usp=drive_link, https://drive.google.com/file/d/1nZmkz0DnqU-sf1WyFFJXlSvSVtOWw9kF/view?usp=drive_link, https://drive.google.com/file/d/1YMVXojJXvRxr0Vw9Um1nYeeZsJCA7XPI/view?usp=drive_link, https://drive.google.com/file/d/111SSNL0guS9RqAtvvilNn-3eiCpz8lPd/view?usp=drive_link, https://drive.google.com/file/d/1EqqWDVh_tZDztf9z-8vdqN0gavDAFgzz/view?usp=drive_link, https://drive.google.com/file/d/1nNyl_qhMw8OGcjjk9zM7msR3AZZI8YxZ/view?usp=drive_link, https://drive.google.com/file/d/1iLEkS298wtsAXMDNKkfaKivMcwIQTnBg/view?usp=drive_link, https://drive.google.com/file/d/120HePK89nNTsnMFfuoa2ObRZNmtDj9-x/view?usp=drive_link, https://drive.google.com/file/d/1Mv6aml9ALaOc6pdYqQA1mjEM3-qCvHHq/view?usp=drive_link, https://drive.google.com/file/d/1V8BUICljb-JJZYjN5uAc2RTSSz-tvJ5E/view?usp=drive_link, https://drive.google.com/file/d/1n5qmwAvA7_-7V2qMG4kBluJmzOxLgap2/view?usp=drive_link, https://drive.google.com/file/d/1rQRsARrOb9RE10_cHCywyvPnsdzAC-rI/view?usp=drive_link, https://drive.google.com/file/d/11rePV1AkDGGLBfHAd7WqOPQ6ItVdReji/view?usp=drive_link, https://drive.google.com/file/d/1ni4vrgvi-i37lZJ9RtL4d4DAmkVgGPvm/view?usp=drive_link, https://drive.google.com/file/d/1yX7w0WEhHz8uitAG1szD0oqKkiIBKQ_9/view?usp=drive_link, https://drive.google.com/file/d/1aoAR_azdO9_jIbfPzlW45ID_ZVqY-6hb/view?usp=drive_link, https://drive.google.com/file/d/1tGCBP0GaBkd6vqXluHvcW1bNZNVMcgU2/view?usp=drive_link, https://drive.google.com/file/d/1gC0jE0BtqnvwVZkfosDkucauRriNRhUC/view?usp=drive_link, https://drive.google.com/file/d/1kbAvEFlfB1LuabqdKd7jgtp2Mbq_iELt/view?usp=drive_link, https://drive.google.com/file/d/1MCV-CGn6qLKfpqd638fHX4-O2EvS07u-/view?usp=drive_link, https://drive.google.com/file/d/1UJFONTpC78P3B3qVvF2jmvCu2t-9fCpt/view?usp=drive_link, https://drive.google.com/file/d/1Dx9d22kohapDbHsBM0x6yYhR4jMQ42q5/view?usp=drive_link, https://drive.google.com/file/d/1T2Psb5nTil2i68UY1vtIZ1pAGcP5THXH/view?usp=drive_link, https://drive.google.com/file/d/11RCxwq9n__gvQ-2Qb-U0vLTLCvensVT1/view?usp=drive_link, https://drive.google.com/file/d/1k4W5Gs6MVgQXCND6TPjPsLPQmVp-dx5j/view?usp=drive_link, https://drive.google.com/file/d/1OpJ2YU0V1BH594PKZ2C41yAv9tNb0cVi/view?usp=drive_link, https://drive.google.com/file/d/1zijyT1eb_bUOxBGjIrWZ32vadrtvb4rg/view?usp=drive_link, https://drive.google.com/file/d/1zK-Ko5psqpEGAQeou2_m5d4aJVb515e8/view?usp=drive_link, https://drive.google.com/file/d/1IcvvFcOBe1LfURnmnSTmOZ5aMLaH4pfL/view?usp=drive_link, https://drive.google.com/file/d/1RQGJRPQvqQ4ai7-GXvghNJdyblDPwXmm/view?usp=drive_link, https://drive.google.com/file/d/15Q5Qt8jGzUV0j9TyOMpm4q6MQeEMQTga/view?usp=drive_link, https://drive.google.com/file/d/1PAbuvZeHcic2cGc9508LuFkJbcZah9uW/view?usp=drive_link, https://drive.google.com/file/d/1HNONC-iUQ-GT8Vi5M6WbUcC5BkwzD93N/view?usp=drive_link, https://drive.google.com/file/d/1XglQQQ4UO5mU7Loenh-bKK7I2LM4vFNA/view?usp=drive_link, https://drive.google.com/file/d/1ZNF9fQwvUk2CWxZTTGjrpgpsn8mvwhoG/view?usp=drive_link, https://drive.google.com/file/d/1PzOX_JnqMnCgQt0XXWQgMsBG1kBCvOgp/view?usp=drive_link, https://drive.google.com/file/d/1v_xU8UkaxxZOjb13N2ByacYhW_oVJLdb/view?usp=drive_link, https://drive.google.com/file/d/1dZXEWk5Lo-p07iEzugoUfYxz8p_1BuXc/view?usp=drive_link, https://drive.google.com/file/d/1fbeodr2GedJuHPRqtKvQXL3Z92ynSXgB/view?usp=drive_link, https://drive.google.com/file/d/1q0th6-Wmtn9knQjWjv4svPGHxBBWFvLr/view?usp=drive_link, https://drive.google.com/file/d/17rF2fDYGefedUClfEm8rtHL_Zb4ZHViT/view?usp=drive_link, https://drive.google.com/file/d/1cQKBFVCyQ9WrurFpe35R5dOEXlDql8g7/view?usp=drive_link, https://drive.google.com/file/d/1696SSzCberEhpP2CHn9OVTdbGaP7ilq8/view?usp=drive_link, https://drive.google.com/file/d/1WzTqkJMD27HW9vCXf5iCXminWt_UzPqR/view?usp=drive_link, https://drive.google.com/file/d/1drtzZEF_840JXb9GyoojpZ7sD7R3vg4C/view?usp=drive_link, https://drive.google.com/file/d/17XSjzoEt2rtrI0H8as_VGr16IAV1h1Oz/view?usp=drive_link, https://drive.google.com/file/d/1EG30NJr5fN6a4Dj8Of8CTc9xr-l3DR_b/view?usp=drive_link, https://drive.google.com/file/d/1H10xpqrF7NSxPgpJcasdMhW37YBvfYz8/view?usp=drive_link, https://drive.google.com/file/d/1iey0JfIH4JDZlwBZrSyiZN8A_XQW2A2O/view?usp=drive_link, https://drive.google.com/file/d/1IuAUYy8bQBA9Rr0LYGmlK6frQTpxkECN/view?usp=drive_link, https://drive.google.com/file/d/1XfkTWWfKgiBOw0mEJZKlDu5I5JFDdBr2/view?usp=drive_link, https://drive.google.com/file/d/1a35nw9aw3eqpFejGtLdOXEvDcN3c9vYQ/view?usp=drive_link, https://drive.google.com/file/d/1pN-BaLf7RPj9IX6I-o0LOmYl5I-CuxU5/view?usp=drive_link, https://drive.google.com/file/d/1rV4VyAzwt6cbc8qJbzUVz-jAe3Mh8zAh/view?usp=drive_link, https://drive.google.com/file/d/1b33Sp0Hn1KQuVv9xO2IwyU1FXhTZmg9_/view?usp=drive_link'
# Split the string into individual URLs
urls = image_share_links.split(", ")

# Extract the IDs from each URL
image_ids = [str(extract_id_from_url(url)) for url in urls]
softmax_logits = []
for image_id in image_ids:
  softmax_logit = test_on_new_image(image_id)
  softmax_logits.append(softmax_logit)
softmax_logits
np.max(softmax_logits)
np.min(softmax_logits)
np.mean(softmax_logits)

## Experiments without Background Removal
import requests
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import torch


def test_on_new_image(id):
  # Load image
  url = "https://drive.usercontent.google.com/download?id=" + id
  response = requests.get(url, stream=True)
  img = Image.open(response.raw)

  plt.title("Original Image")
  plt.imshow(img)
  plt.axis("off")  # Hide axes
  plt.show()

  image_array = process_image_backgroundincluded(url, 224, 224)
  plt.imshow(image_array)

  # Convert the image array to a PyTorch tensor
  image_tensor = torch.tensor(image_array, dtype=torch.float32)

  # Convert the image tensor back to a NumPy array
  image_array = image_tensor.numpy()

  # Display the image
  plt.imshow(image_array)
  plt.show()

  # Convert image to tensor and transpose dimensions
  image_tensor_unsqueezed = image_tensor.unsqueeze(0)
  image_tensor_transposed = np.transpose(image_tensor_unsqueezed, (0, 3, 1, 2))

  # Perform prediction
  with torch.no_grad():
      input = torch.tensor(image_tensor_transposed, dtype=torch.float32).to(device)
      output = vit_model(input)
      logits = output.logits
      predicted_label = torch.argmax(logits, 1)
      softmax_probs = torch.softmax(logits, 1)
      softmax_prob = softmax_probs[0, predicted_label].cpu().numpy()

  # Retrieve string for predicted label
  subcategory = labels[predicted_label]

  # Retrieve general recycling category for predicted label
  gen_label = general_recycling[subcategory]

  # Set is_recyclable
  is_recyclable = subcategory != 'trash'

  # Subcategory recommendation
  subcategory_sug = "trash" if softmax_prob <= 0.5 else subcategory

  # Set confidence category
  if softmax_prob <= 0.5:
      confidence_category = "low"
  elif 0.5 < softmax_prob < 0.75:
      confidence_category = "medium"
  else:
      confidence_category = "high"

  print("image_url: ", url)
  # print("filename: ", filename)
  print("wastetype_raw: ", subcategory)
  print("wastetype_suggested: ", subcategory_sug)
  print("wastetype_general: ", gen_label)
  print("is_recyclable: ", is_recyclable)
  print("confidence_category: ", confidence_category)
  print("softmax_logit: ", np.round(softmax_prob, 6)[0])

  return np.round(softmax_prob, 6)[0]

import re

results = []
labels = ['battery', 'beverage cans', 'cardboard', 'cigarette butt',
       'construction scrap', 'electrical cables', 'electronic chips',
       'glass', 'gloves', 'laptops', 'masks', 'medicines',
       'metal containers', 'news paper', 'paper',
       'paper_cups', 'plastic bags', 'plastic bottles',
       'plastic containers', 'plastic_cups', 'small appliances',
       'smartphones', 'spray cans', 'syringe', 'tetra pak', 'trash']
general_recycling = {
    'battery': 'battery',
    'beverage cans': 'metal',
    'cardboard': 'cardboard',
    'cigarette butt': 'plastic',
    'construction scrap': 'metal',
    'electrical cables': 'e-waste',
    'electronic chips': 'e-waste',
    'glass': 'glass',
    'gloves': 'medical',
    'laptops': 'e-waste',
    'masks': 'medical',
    'medicines': 'medical',
    'metal containers': 'metal',
    'news paper': 'paper',
    'paper': 'paper',
    'paper_cups': 'paper',
    'plastic bags': 'plastic',
    'plastic bottles': 'plastic',
    'plastic containers': 'plastic',
    'plastic_cups': 'plastic',
    'small appliances': 'e-waste',
    'smartphones': 'e-waste',
    'spray cans': 'metal',
    'syringe': 'medical',
    'tetra pak': 'paper',
    'trash': 'trash'
}

def extract_id_from_url(url):
    # Regex pattern to match the ID between 'd/' and '/view?'
    pattern = r'd/([^/]+)'
    match = re.search(pattern, url)
    # If a match is found, return the ID group
    if match:
        return match.group(1)
    else:
        return None

image_share_links = 'https://drive.google.com/file/d/1pwuQ3o7Duan3H94djfi7IBrUcIYemOj7/view?usp=drive_link, https://drive.google.com/file/d/14sCAj6-YY8otCHIgv_CzGj0vr0EnIXTK/view?usp=drive_link, https://drive.google.com/file/d/1sVIAcL3TKjTO0wQG22q-fDN-wpX_ugxc/view?usp=drive_link, https://drive.google.com/file/d/1ZtvcWuD1MIOGV4Seglst_1Q_zck5VPx2/view?usp=drive_link, https://drive.google.com/file/d/1GWSlC_y35PgbVBWerXmG5J9MwdTlBQ2E/view?usp=drive_link, https://drive.google.com/file/d/1TgtLA0AjBq6COc2MLjHtt_ATLSz_n2rw/view?usp=drive_link, https://drive.google.com/file/d/1dnIjEAyvzxV8UqvXipV_XCrmXtY2lIN0/view?usp=drive_link, https://drive.google.com/file/d/1K-NS3FyQaR8eJYQ6xc-XCxv1_JdoUeJn/view?usp=drive_link, https://drive.google.com/file/d/1pKwNyxjPr6V9uK9Ni__vJZP1lseWbltD/view?usp=drive_link, https://drive.google.com/file/d/1fVTL3EPIKCKR3I2ad1B9venKcCyjWRg6/view?usp=drive_link, https://drive.google.com/file/d/16gvLK9PF3HFBcAbHre0zGEol6WxVKDva/view?usp=drive_link, https://drive.google.com/file/d/1OUZPC08Ll1EtVG1xpTMEN8p1VZ9eRYF0/view?usp=drive_link, https://drive.google.com/file/d/1xbqroXJMBH_fNNd-NCnaSTCNY_VAT6SW/view?usp=drive_link, https://drive.google.com/file/d/185-1J1VHWWeeTjgXF6-6qSyZckHLztVd/view?usp=drive_link, https://drive.google.com/file/d/1l6p1kikwIynOqg0MUFUjh099OaBqYFBO/view?usp=drive_link, https://drive.google.com/file/d/1Nc5GvQogY4dhB0f6DiozjEmMsPdbupga/view?usp=drive_link, https://drive.google.com/file/d/1dvHUPdZeuFiR1Kv27WV-KN-klm8pnKR7/view?usp=drive_link, https://drive.google.com/file/d/1s3hG7G8AGOrNFyoSpvGUynEtMM8SIh9k/view?usp=drive_link, https://drive.google.com/file/d/1ryMZSrVpmecXFB8BlFlFgMJJyqC6Tf8N/view?usp=drive_link, https://drive.google.com/file/d/1WyBgbChLyTqjTBrq6h96lI4ZSMGq5laW/view?usp=drive_link, https://drive.google.com/file/d/145o-59Tw-y7bq9RHtUh33u_QLDW9Wq7Y/view?usp=drive_link, https://drive.google.com/file/d/1iPHilOm2BWbDCVyFf0eHVnKHiVWHCgPo/view?usp=drive_link, https://drive.google.com/file/d/1gssBv5lrrsunhZODPQw4l4ey8CezLFF3/view?usp=drive_link, https://drive.google.com/file/d/1OJntNLmV9wZURCnXWSURl9oYUnQ5WU9e/view?usp=drive_link, https://drive.google.com/file/d/1wEAn3S3-BABIXDPOJkPeP2iU6TW1L7mH/view?usp=drive_link, https://drive.google.com/file/d/1cq_bpyryPGpY_U99sP2Gjj9CK0avwkwG/view?usp=drive_link, https://drive.google.com/file/d/1JngdW7MnHXBBZhMeIzPpST4z4NpMs3Gc/view?usp=drive_link, https://drive.google.com/file/d/18yBxjwYk_Qc6U_TtrS4iUdyGCPxdXElP/view?usp=drive_link, https://drive.google.com/file/d/1luGTctak7Hg2yAWtFFkhUFB1s9QKCxx8/view?usp=drive_link, https://drive.google.com/file/d/1fo8ue-Hr4G8Q-8wiOeJZCEa9ZtqfFU8B/view?usp=drive_link, https://drive.google.com/file/d/1OGOnNi-3mqE9J2UUtI3MPrvFUxuqP7ar/view?usp=drive_link, https://drive.google.com/file/d/1bIfE8Q3XeJkTSPTVhSrL4timKLAvqNZz/view?usp=drive_link, https://drive.google.com/file/d/1Dhpjjlt9ITTSLUQQtjiEbbzGG20lK0I1/view?usp=drive_link, https://drive.google.com/file/d/1TSCM1qViy2drPxl1wO0eFU9nvxK5w79N/view?usp=drive_link, https://drive.google.com/file/d/1DgpnEoCbeRLw-UbbiaIAK5oaHg_bULq4/view?usp=drive_link, https://drive.google.com/file/d/1LXxUhJuxU7kFpLJBR8yjUxpxnM5sccZ3/view?usp=drive_link, https://drive.google.com/file/d/1_FB357gIuQDDKM6cMvZMw_7XeGXFmafa/view?usp=drive_link, https://drive.google.com/file/d/1qkjm5uLXeM-mrd1V1tZNC9t8PvlVm6Kg/view?usp=drive_link, https://drive.google.com/file/d/19-EqEaqBiiKdQ4fUi7VHLxIK726KmkLS/view?usp=drive_link, https://drive.google.com/file/d/1oZtQK9cGPikJt-pC-iT76-L5Q_L7ik_G/view?usp=drive_link, https://drive.google.com/file/d/10kn8e2HoX9752xQyUFDPgpqRVg2LwbAQ/view?usp=drive_link, https://drive.google.com/file/d/1ZXXDPiy18lkeTwYl1ThPb6WSiFWQix68/view?usp=drive_link, https://drive.google.com/file/d/1tiEo9l822N4EO4idDjH91_wqXdq9KRCT/view?usp=drive_link, https://drive.google.com/file/d/1o3I46H5oZLEl1xOI4e3n947IgtP7nAfO/view?usp=drive_link, https://drive.google.com/file/d/1uTE_3m_6VJ4r3vcVaAr7YpuQ3GRm1mpN/view?usp=drive_link, https://drive.google.com/file/d/1xyVpmDhD5ugIgLVjihBy7RoExGt24G0G/view?usp=drive_link, https://drive.google.com/file/d/19mXdguQN3EF7NUVQplTfqSX0LvMROY18/view?usp=drive_link, https://drive.google.com/file/d/1ZmerORshOWfS_eKCyWGYv9MVQdnvj0BS/view?usp=drive_link, https://drive.google.com/file/d/1N5DkliGUU9yZ2sJZM4MciNl5W42x-Fff/view?usp=drive_link, https://drive.google.com/file/d/1xsekKFqq_tmw1hxyc9gCspZVnrg74Ual/view?usp=drive_link, https://drive.google.com/file/d/1LemycgmJ6IZubj-VxXnxUhWFqg-cud2B/view?usp=drive_link, https://drive.google.com/file/d/1HsD6M1D4ZBaI5AONlnL6x5bSzWFbhnI5/view?usp=drive_link, https://drive.google.com/file/d/1rRjoKaJf9xUtu-CEEKG5c4X9L6nhfJR2/view?usp=drive_link, https://drive.google.com/file/d/1P00y728UWNWSHySp8d4d8QSyVNPyl8K5/view?usp=drive_link, https://drive.google.com/file/d/1ByYx-8gKIL2V-v2rg8L9jaciy7RYyqNk/view?usp=drive_link, https://drive.google.com/file/d/17_Dp-UxIuaMNNBL7mbwYoL8PmEd4vYdv/view?usp=drive_link, https://drive.google.com/file/d/1Fvg-SukRpINrxU2ZE9tsoUnrKUnxrRoe/view?usp=drive_link, https://drive.google.com/file/d/1oFc-SlyuPUBLCLpDCTC7eMjL716GQdz5/view?usp=drive_link, https://drive.google.com/file/d/1vs_S1aqsfAgXxuNL8kxOQsYuBKvuRDos/view?usp=drive_link, https://drive.google.com/file/d/1Ue4iyNczJxjspd4XA_23KJexLfUwiQop/view?usp=drive_link, https://drive.google.com/file/d/1ObJAZFhQUGerGi117PKkksbsHSvkFMRh/view?usp=drive_link, https://drive.google.com/file/d/1eDXobHrbaZQGUQw0M6ooHeDG-4RyYZ-v/view?usp=drive_link, https://drive.google.com/file/d/1P4_xjMBdKLfoEo-4kvFeGfJrl-L_KlAU/view?usp=drive_link, https://drive.google.com/file/d/1vPV6AitgwiwZ0WfduCan5jhtFZrLqfnc/view?usp=drive_link, https://drive.google.com/file/d/1uHPQaNfA3yiz0q-ahfUEFM4dmckuI8S9/view?usp=drive_link, https://drive.google.com/file/d/1nZmkz0DnqU-sf1WyFFJXlSvSVtOWw9kF/view?usp=drive_link, https://drive.google.com/file/d/1YMVXojJXvRxr0Vw9Um1nYeeZsJCA7XPI/view?usp=drive_link, https://drive.google.com/file/d/111SSNL0guS9RqAtvvilNn-3eiCpz8lPd/view?usp=drive_link, https://drive.google.com/file/d/1EqqWDVh_tZDztf9z-8vdqN0gavDAFgzz/view?usp=drive_link, https://drive.google.com/file/d/1nNyl_qhMw8OGcjjk9zM7msR3AZZI8YxZ/view?usp=drive_link, https://drive.google.com/file/d/1iLEkS298wtsAXMDNKkfaKivMcwIQTnBg/view?usp=drive_link, https://drive.google.com/file/d/120HePK89nNTsnMFfuoa2ObRZNmtDj9-x/view?usp=drive_link, https://drive.google.com/file/d/1Mv6aml9ALaOc6pdYqQA1mjEM3-qCvHHq/view?usp=drive_link, https://drive.google.com/file/d/1V8BUICljb-JJZYjN5uAc2RTSSz-tvJ5E/view?usp=drive_link, https://drive.google.com/file/d/1n5qmwAvA7_-7V2qMG4kBluJmzOxLgap2/view?usp=drive_link, https://drive.google.com/file/d/1rQRsARrOb9RE10_cHCywyvPnsdzAC-rI/view?usp=drive_link, https://drive.google.com/file/d/11rePV1AkDGGLBfHAd7WqOPQ6ItVdReji/view?usp=drive_link, https://drive.google.com/file/d/1ni4vrgvi-i37lZJ9RtL4d4DAmkVgGPvm/view?usp=drive_link, https://drive.google.com/file/d/1yX7w0WEhHz8uitAG1szD0oqKkiIBKQ_9/view?usp=drive_link, https://drive.google.com/file/d/1aoAR_azdO9_jIbfPzlW45ID_ZVqY-6hb/view?usp=drive_link, https://drive.google.com/file/d/1tGCBP0GaBkd6vqXluHvcW1bNZNVMcgU2/view?usp=drive_link, https://drive.google.com/file/d/1gC0jE0BtqnvwVZkfosDkucauRriNRhUC/view?usp=drive_link, https://drive.google.com/file/d/1kbAvEFlfB1LuabqdKd7jgtp2Mbq_iELt/view?usp=drive_link, https://drive.google.com/file/d/1MCV-CGn6qLKfpqd638fHX4-O2EvS07u-/view?usp=drive_link, https://drive.google.com/file/d/1UJFONTpC78P3B3qVvF2jmvCu2t-9fCpt/view?usp=drive_link, https://drive.google.com/file/d/1Dx9d22kohapDbHsBM0x6yYhR4jMQ42q5/view?usp=drive_link, https://drive.google.com/file/d/1T2Psb5nTil2i68UY1vtIZ1pAGcP5THXH/view?usp=drive_link, https://drive.google.com/file/d/11RCxwq9n__gvQ-2Qb-U0vLTLCvensVT1/view?usp=drive_link, https://drive.google.com/file/d/1k4W5Gs6MVgQXCND6TPjPsLPQmVp-dx5j/view?usp=drive_link, https://drive.google.com/file/d/1OpJ2YU0V1BH594PKZ2C41yAv9tNb0cVi/view?usp=drive_link, https://drive.google.com/file/d/1zijyT1eb_bUOxBGjIrWZ32vadrtvb4rg/view?usp=drive_link, https://drive.google.com/file/d/1zK-Ko5psqpEGAQeou2_m5d4aJVb515e8/view?usp=drive_link, https://drive.google.com/file/d/1IcvvFcOBe1LfURnmnSTmOZ5aMLaH4pfL/view?usp=drive_link, https://drive.google.com/file/d/1RQGJRPQvqQ4ai7-GXvghNJdyblDPwXmm/view?usp=drive_link, https://drive.google.com/file/d/15Q5Qt8jGzUV0j9TyOMpm4q6MQeEMQTga/view?usp=drive_link, https://drive.google.com/file/d/1PAbuvZeHcic2cGc9508LuFkJbcZah9uW/view?usp=drive_link, https://drive.google.com/file/d/1HNONC-iUQ-GT8Vi5M6WbUcC5BkwzD93N/view?usp=drive_link, https://drive.google.com/file/d/1XglQQQ4UO5mU7Loenh-bKK7I2LM4vFNA/view?usp=drive_link, https://drive.google.com/file/d/1ZNF9fQwvUk2CWxZTTGjrpgpsn8mvwhoG/view?usp=drive_link, https://drive.google.com/file/d/1PzOX_JnqMnCgQt0XXWQgMsBG1kBCvOgp/view?usp=drive_link, https://drive.google.com/file/d/1v_xU8UkaxxZOjb13N2ByacYhW_oVJLdb/view?usp=drive_link, https://drive.google.com/file/d/1dZXEWk5Lo-p07iEzugoUfYxz8p_1BuXc/view?usp=drive_link, https://drive.google.com/file/d/1fbeodr2GedJuHPRqtKvQXL3Z92ynSXgB/view?usp=drive_link, https://drive.google.com/file/d/1q0th6-Wmtn9knQjWjv4svPGHxBBWFvLr/view?usp=drive_link, https://drive.google.com/file/d/17rF2fDYGefedUClfEm8rtHL_Zb4ZHViT/view?usp=drive_link, https://drive.google.com/file/d/1cQKBFVCyQ9WrurFpe35R5dOEXlDql8g7/view?usp=drive_link, https://drive.google.com/file/d/1696SSzCberEhpP2CHn9OVTdbGaP7ilq8/view?usp=drive_link, https://drive.google.com/file/d/1WzTqkJMD27HW9vCXf5iCXminWt_UzPqR/view?usp=drive_link, https://drive.google.com/file/d/1drtzZEF_840JXb9GyoojpZ7sD7R3vg4C/view?usp=drive_link, https://drive.google.com/file/d/17XSjzoEt2rtrI0H8as_VGr16IAV1h1Oz/view?usp=drive_link, https://drive.google.com/file/d/1EG30NJr5fN6a4Dj8Of8CTc9xr-l3DR_b/view?usp=drive_link, https://drive.google.com/file/d/1H10xpqrF7NSxPgpJcasdMhW37YBvfYz8/view?usp=drive_link, https://drive.google.com/file/d/1iey0JfIH4JDZlwBZrSyiZN8A_XQW2A2O/view?usp=drive_link, https://drive.google.com/file/d/1IuAUYy8bQBA9Rr0LYGmlK6frQTpxkECN/view?usp=drive_link, https://drive.google.com/file/d/1XfkTWWfKgiBOw0mEJZKlDu5I5JFDdBr2/view?usp=drive_link, https://drive.google.com/file/d/1a35nw9aw3eqpFejGtLdOXEvDcN3c9vYQ/view?usp=drive_link, https://drive.google.com/file/d/1pN-BaLf7RPj9IX6I-o0LOmYl5I-CuxU5/view?usp=drive_link, https://drive.google.com/file/d/1rV4VyAzwt6cbc8qJbzUVz-jAe3Mh8zAh/view?usp=drive_link, https://drive.google.com/file/d/1b33Sp0Hn1KQuVv9xO2IwyU1FXhTZmg9_/view?usp=drive_link'
# Split the string into individual URLs
urls = image_share_links.split(", ")

# Extract the IDs from each URL
image_ids = [str(extract_id_from_url(url)) for url in urls]
softmax_logits_backgroundincluded = []
for image_id in image_ids:
  softmax_logit = test_on_new_image(image_id)
  softmax_logits_backgroundincluded.append(softmax_logit)
np.max(softmax_logits_backgroundincluded)
np.min(softmax_logits_backgroundincluded)
?np.mean(softmax_logits_backgroundincluded)

